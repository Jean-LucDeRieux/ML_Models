{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Binary Cross Entropy) #\n",
    "\n",
    "## Model ##\n",
    "$g\\left(z\\right)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "$f_{\\vec{\\omega}\\ ,b}\\left(\\vec{x}\\right)=g\\left(\\vec{\\omega}\\cdot\\vec{x}+b\\right)=\\frac{1}{1+e^{-\\left(\\vec{\\omega}\\cdot\\vec{x}+b\\right)}}$\n",
    "\n",
    "## Cost / Loss Functions ##\n",
    "$loss\\left(f_{\\vec{\\omega},b}\\left(x^{\\left(i\\right)}\\right),y^{\\left(i\\right)}\\right)=-y^{\\left(i\\right)}\\log{\\left(f_{\\vec{\\omega},b}\\left({\\vec{x}}^{\\left(i\\right)}\\right)\\right)}-\\left(1-y^{\\left(i\\right)}\\right)\\log{\\left(1-f_{\\vec{\\omega},b}\\left({\\vec{x}}^{\\left(i\\right)}\\right)\\right)}$\n",
    "\n",
    "$J\\left(\\vec{\\omega},b\\right)=\\frac{1}{m}\\sum_{i=0}^{m-1}\\left[loss\\left(f_{\\vec{\\omega},b}\\left({\\vec{x}}^{\\left(i\\right)}\\right),y^{\\left(i\\right)}\\right)\\right]$\n",
    "\n",
    "## Gradient ##\n",
    "$\\frac{\\partial J\\left(\\vec{\\omega},b\\right)}{\\partial\\omega}=\\frac{1}{m}\\sum_{i=0}^{m-1}\\left[\\left(f_{\\vec{\\omega},b}\\left({\\vec{x}}^{\\left(i\\right)}\\right)-y^{\\left(i\\right)}\\right){\\vec{x}}^{\\left(i\\right)}\\right]$\n",
    "\n",
    "$\\frac{\\partial J\\left(\\vec{\\omega},b\\right)}{\\partial b}=\\frac{1}{m}\\sum_{i=0}^{m-1}\\left[f_{\\vec{\\omega},b}\\left({\\vec{x}}^{\\left(i\\right)}\\right)-y^{\\left(i\\right)}\\right]$\n",
    "\n",
    "## Gradient Descent ##\n",
    "$\\vec{\\omega}'=\\vec{\\omega}-\\alpha\\frac{\\partial J\\left(\\omega,b\\right)}{\\partial\\omega}$\n",
    "\n",
    "$b'=b-\\alpha\\frac{\\partial J(\\omega,b)}{\\partial b}$\n",
    "\n",
    "Alternatively notation,\n",
    "\n",
    "$\\vec{\\omega}:=\\vec{\\omega}-\\alpha\\frac{\\partial J\\left(\\omega,b\\right)}{\\partial\\omega}$\n",
    "\n",
    "$b:= b-\\alpha\\frac{\\partial J(\\omega,b)}{\\partial b}$\n",
    "\n",
    "## Regularized Linear Regression ##\n",
    "$\\frac{\\partial J\\left(\\vec{\\omega},b\\right)}{\\partial\\omega}=\\frac{1}{m}\\sum_{i=0}^{m-1}{\\left[\\left(f_{\\vec{\\omega},b}\\left({\\vec{x}}^{\\left(i\\right)}\\right)-y^{\\left(i\\right)}\\right){\\vec{x}}^{\\left(i\\right)}\\right]+\\frac{\\lambda}{m}\\vec{\\omega}}$\n",
    "\n",
    "$\\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}{[\\vec{\\omega _ j}]}^2$\n",
    "# Misc #\n",
    "Feature Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, X, y, b):\n",
    "        self.input = X\n",
    "        self.y = y\n",
    "        self.weights = np.random.rand(X.shape[1], 1)\n",
    "        self.bias = b\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = X @ self.weights + self.bias\n",
    "        return y_hat\n",
    "    \n",
    "    def sigmoid(z):\n",
    "        g = (1 / (1 + np.exp(-z)))\n",
    "        return g\n",
    "\n",
    "    def costFunc(self, X, y, w, b):\n",
    "        m, n = X.shape\n",
    "        z = (np.dot(X,w) + b)\n",
    "        sigmoid_vectorized = np.vectorize(self.sigmoid)\n",
    "        f_wb = sigmoid_vectorized(z)\n",
    "        total_loss = (- y * np.log(f_wb)) - ((1 - y) * np.log(1 - f_wb))\n",
    "        total_cost = (np.sum(total_loss) / m )\n",
    "        return total_cost\n",
    "\n",
    "    def gradientFunc(self, X, y, w, b):\n",
    "        m = len(X)\n",
    "        gradient = (1/m)*(np.sum((np.dot(X,w) + b - y) @ X)) # (1/m) * ((10,4)(4,1) + b - (10,)) *(10,4)\n",
    "        \n",
    "        f_wb = (np.dot(X, w) + b)\n",
    "        dj_db = np.sum(f_wb - y) / m \n",
    "        dj_dw = np.dot(X.T, (f_wb - y)) / m\n",
    "    \n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def gradientDescentFunc(self, X, y, w, b, lr):\n",
    "        gradDec = w - lr*self.gradientFunc(X, y, w, b)\n",
    "        return gradDec\n",
    "\n",
    "    def train(self, epochs, lr):\n",
    "        for index in range(epochs):\n",
    "            self.weights = self.gradientDescentFunc(self.input, self.y, self.weights, self.bias, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
