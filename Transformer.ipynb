{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "Original Work: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Main Architecture\n",
    "\n",
    "* The Encoder\n",
    "   - Purpose: Processes input sequences and generates a contextual representation.\n",
    "   - The encoder's output is a tensor that contains contextual information about the input sequence, such as word position, part of speech, and meaning.\n",
    "* The Decoder\n",
    "   - Purpose: Generates output sequences, one token at a time, using the contextual representation from the encoder and the tokens generated so far.\n",
    "\n",
    "<!-- ![Transformer Model Architecture](./Transformer-Model-Architecture.jpg) -->\n",
    "\n",
    "### Types of Inputs and Outputs\n",
    "\n",
    "1. **Sequence to Vector**: \n",
    "   - **Example**: Image to description of the image.\n",
    "2. **Sequence to Vector**: \n",
    "   - **Example**: Movie review with an output of a vector of probabilities.\n",
    "3. **Sequence to Sequence**: \n",
    "   - **Example**: Translation from one language to another. Spanish to English as an example.\n",
    "\n",
    "### Vocabulary\n",
    "\n",
    "- **Input Embeddings**: Translates words to vector mappings, locating similar words in an embedding space.\n",
    "- **Positional Encoders**: Provide context based on the position of words in a sentence using sine and cosine functions to help the model understand context.\n",
    "- **Attention**: Focuses on relevant parts of the input, producing an attention vector that captures contextual relationships.\n",
    "- **Tokens**: Each token (word, subword, character, etc.) is represented as a vector of real numbers. \n",
    "\n",
    "### Steps Through the Encoder Block\n",
    "\n",
    "* For Large Language Models (LLMs): Encoders (Input Sequence)\n",
    "   - **Input Embedding**: Maps the sequence of words to a vector space.\n",
    "   - **Positional Encoding**: Encodes the contextual relationship between the current word and its neighboring words.\n",
    "   - **Multi-Head Attention**: Captures the contextual relationship between words in the sentence, computing weighted averages to determine the importance of each word in the sequence.\n",
    "   - **Feed Forward Network**: Processes the attention vectors.\n",
    "\n",
    "* For Large Language Models (LLMs): Decoders (Desired Output Sequence)\n",
    "   - **Input Embedding**: Maps the sequence of words to a vector space.\n",
    "   - **Positional Encoding**: Encodes the contextual relationship between the current word and its neighboring words.\n",
    "   - **Masked Multi-Head Attention**: Uses all words from the input sequence and only previous words from the output sequence to capture contextual relationships, creating vectors for each word.\n",
    "   - **Encoder-Decoder Attention**: Integrates input-output interactions from the encoder and the decoder, encapsulating the relationship between each word.\n",
    "   - **Feed Forward Network**: Processes the encoder-decoder attention vectors.\n",
    "   - **Linear Layer**: Feedforward layer with the size of the target language vocabulary.\n",
    "   - **Softmax Layer**: Produces a probability distribution, outputting the word with the highest probability.\n",
    "\n",
    "### Popular Word Embedding Maps\n",
    "\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "* BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "   - Utilizes multiple encoder layers.\n",
    "   - Focuses on understanding the context of words bidirectionally, using an encoder-only architecture.\n",
    "   - **Ideal for**: Tasks requiring deep understanding of input sequences, such as classification, sentiment analysis, and question answering.\n",
    "\n",
    "* GPT (Generative Pretrained Transformer)\n",
    "\n",
    "   - Utilizes multiple decoder layers.\n",
    "   - Focuses on text generation using a unidirectional approach with a decoder-only architecture.\n",
    "   - **Ideal for**: Tasks requiring text generation, such as dialogue systems, story creation, and summarization.\n",
    "\n",
    "* Full Transformer\n",
    "\n",
    "   - Combines both encoder and decoder layers.\n",
    "   - Transforms input sequences into output sequences, leveraging both components of the architecture.\n",
    "   - **Ideal for**: Sequence-to-sequence transformations, such as machine translation and complex text summarization.\n",
    "\n",
    "* Use Cases\n",
    "\n",
    "   - **BERT**:\n",
    "   - Uses the encoder to understand and contextualize input text bidirectionally.\n",
    "   - Suitable for tasks like classification, sentiment analysis, and question answering.\n",
    "\n",
    "   - **GPT**:\n",
    "   - Uses the decoder to generate text based on previous context unidirectionally.\n",
    "   - Suitable for tasks like text generation, dialogue systems, and story creation.\n",
    "\n",
    "   - **Full Transformer**:\n",
    "   - Combines encoder and decoder for transforming input sequences to output sequences.\n",
    "   - Suitable for tasks requiring sequence-to-sequence transformations, like translation.\n",
    "   \n",
    "### Transfer Learning Process\n",
    "\n",
    "1. **Pretraining**: Generates a word given its context (e.g., \"I love\" outputs \"you\").\n",
    "2. **Finetuning**: Trains the model on a specific task."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
